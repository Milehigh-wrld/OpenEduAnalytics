{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Learning Analytics Package: Build Fact Tables\r\n",
        "\r\n",
        "Builds the fact table used for the Learning Analytics package **v1.0** dashboard, in the context of using the Higher Ed. test data from Microsoft Education Insights data, and Microsoft Graph meeting attendance data.\r\n",
        "\r\n",
        "The following tables are created in each of the steps outlined below:\r\n",
        "\r\n",
        "1. fact_Enrollment\r\n",
        "2. fact_MeetingAttendance\r\n",
        "3. fact_Assignment\r\n",
        "4. fact_Activity\r\n",
        "\r\n",
        "This package-notebook also uses two methods (defined and outlined below; same as the LA_build_dimension_tables notebook):\r\n",
        " - **_publish_to_stage2**: uses the OEA_py function *upsert* to land the current dataframe in stage2/Enriched\r\n",
        " - **publish**: uses the method above to land the package fact tables in stage2, then writes the same table to stage3/Published. Delta checkpoints are landed in the respective stage2/Enriched directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:51:34.5151589Z",
              "session_start_time": "2023-01-11T18:51:34.5652712Z",
              "execution_start_time": "2023-01-11T18:52:19.5940683Z",
              "execution_finish_time": "2023-01-11T18:52:19.7932845Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "workspace = 'dev'\r\n",
        "insights_version  = '1.14'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "59",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:51:34.5203575Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:52:20.7775526Z",
              "execution_finish_time": "2023-01-11T18:52:20.7778197Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 59, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 18:52:20,470 - OEA - INFO - Now using workspace: dev\n2023-01-11 18:52:20,472 - OEA - INFO - OEA initialized.\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:51:34.5279831Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:52:20.8782975Z",
              "execution_finish_time": "2023-01-11T18:52:21.0415072Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 18:52:20,864 - OEA - INFO - Now using workspace: dev\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 0.) Define the Publish Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:51:34.545698Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:52:21.2035852Z",
              "execution_finish_time": "2023-01-11T18:52:21.3685559Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def _publish_to_stage2(df, destination, pk):\r\n",
        "    oea.upsert(df, destination, pk)\r\n",
        "\r\n",
        "def publish(df, stage2_destination, stage3_destination, primary_key='id'):\r\n",
        "    _publish_to_stage2(df, stage2_destination, primary_key)\r\n",
        "\r\n",
        "    spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "    streaming_df = spark.readStream.format('delta').load(oea.to_url(stage2_destination))\r\n",
        "    # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\r\n",
        "    query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(stage2_destination) + '/_checkpoints')\r\n",
        "    query = query.start(oea.to_url(stage3_destination))\r\n",
        "    query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "    number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\r\n",
        "    logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\r\n",
        "    logger.debug(query.lastProgress)\r\n",
        "    return number_of_new_inbound_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.) Build fact_Enrollment Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights roster data: Enrollment (as well as others).\r\n",
        "\r\n",
        "This table has one row per student enrollment in a section from the Insights roster data, with details around:\r\n",
        " - enrollment ID (unique ID per student enrolled in a section),\r\n",
        " - school ID (Id from Organization table),\r\n",
        " - course ID (course that the section belongs to),\r\n",
        " - section ID,\r\n",
        " - instructor ID (that section's instructor)\r\n",
        " - student ID (previously, student internal ID or PersonId from Person table), and\r\n",
        " - student section entry and exit date (from Insights Enrollment table).\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/fact_Enrollment```.\r\n",
        "\r\n",
        "**FIX ENTRY AND EXIT DATE COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:52:23.079566Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:52:23.1946075Z",
              "execution_finish_time": "2023-01-11T18:52:54.509421Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfInsights_aadgroup = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadGroup')\r\n",
        "dfInsights_activity = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/activity')\r\n",
        "dfInsights_enrollment = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/Enrollment')\r\n",
        "dfInsights_section = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/Section')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aadgroup_np = oea.load('stage2/Refined/M365/v'+ insights_version +'/sensitive/AadGroup_lookup')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:52:35.1697866Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:52:54.6662662Z",
              "execution_finish_time": "2023-01-11T18:53:01.6398427Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "59fc7326-631f-48c0-87db-f4985d5f9b4e",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 59fc7326-631f-48c0-87db-f4985d5f9b4e)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# NOTE: Fix entry and exit date column-processing, and clean up curation process below.\r\n",
        "# extract the classes initally provided from the Insights Activity test data, and extract the Teacher IDs \r\n",
        "dfEnroll = dfInsights_activity.filter(dfInsights_activity['ActorRole'] != 'Student')\r\n",
        "dfEnroll = dfEnroll.filter(dfEnroll['ActorRole'] != 'ActorRole')\r\n",
        "dfEnroll = dfEnroll.select('ClassId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "dfEnroll = dfEnroll.groupBy('ClassId', 'ActorId_pseudonym', 'ActorRole').count()\r\n",
        "dfEnroll = dfEnroll.withColumnRenamed('ClassId', 'AADGroup_ClassId').withColumnRenamed('ActorId_pseudonym', 'InstructorId_pseudonym').withColumnRenamed('ActorRole', 'PersonRole')\r\n",
        "dfEnroll = dfEnroll.drop('count')\r\n",
        "# join the Insights AADGroup_pseudo and _lookup table to the dfEnroll table, to provide mapping of the hashed and non-hashed AADGroup Class IDs\r\n",
        "dfInsights_aadgroup_np_ = dfInsights_aadgroup_np.select('ObjectId', 'DisplayName', 'ObjectId_pseudonym').withColumnRenamed('ObjectId', 'Id').withColumnRenamed('ObjectId_pseudonym', 'AADGroup_ClassId_pseudonym')\r\n",
        "dfEnroll = dfEnroll.join(dfInsights_aadgroup_np_, dfEnroll.AADGroup_ClassId == dfInsights_aadgroup_np_.Id, how='inner')\r\n",
        "dfEnroll = dfEnroll.drop('Id')\r\n",
        "dfInsights_aadgroup_ = dfInsights_aadgroup.select('ObjectId_pseudonym', 'SectionId')\r\n",
        "dfEnroll = dfEnroll.join(dfInsights_aadgroup_, dfEnroll.AADGroup_ClassId_pseudonym == dfInsights_aadgroup_.ObjectId_pseudonym, how='inner')\r\n",
        "dfEnroll = dfEnroll.drop('ObjectId_pseudonym', 'PersonRole', 'DisplayName', 'AADGroup_ClassId', 'AADGroup_ClassId_pseudonym')\r\n",
        "# use the Insights Section table to get each associated Course and School ID with a Section\r\n",
        "df_metadata = dfInsights_section.select('Id', 'OrganizationId', 'CourseId')\r\n",
        "dfEnroll = dfEnroll.join(df_metadata, dfEnroll.SectionId == df_metadata.Id, how='inner').drop('Id')\r\n",
        "dfEnroll = dfEnroll.withColumnRenamed('OrganizationId', 'SchoolId')\r\n",
        "# last, use the Insights Enrollment table to get students enrolled within each class\r\n",
        "df_metadata = dfInsights_enrollment.select('Id', 'PersonId_pseudonym', 'RefSectionRoleId', 'SectionId', 'EntryDate', 'ExitDate')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['RefSectionRoleId'] == 'Student')\r\n",
        "df_metadata = df_metadata.drop('RefSectionRoleId').withColumnRenamed('Id', 'EnrollmentId').withColumnRenamed('PersonId_pseudonym', 'StudentId_pseudonym').withColumnRenamed('SectionId', 'id')\r\n",
        "dfEnroll = dfEnroll.join(df_metadata, dfEnroll.SectionId == df_metadata.id, how='inner').drop('id')\r\n",
        "dfEnroll = dfEnroll.select('EnrollmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'EntryDate', 'ExitDate')\r\n",
        "display(dfEnroll.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write to Stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:55:00.0832291Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:55:00.1906418Z",
              "execution_finish_time": "2023-01-11T18:55:12.8510209Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 18:55:11,111 - OEA - INFO - Number of new inbound rows processed: 3600\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "3600"
          },
          "execution_count": 17,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "publish(dfEnroll, 'stage2/Enriched/learning_analytics/v1.0/general/fact_Enrollment', 'stage3/Published/learning_analytics/v1.0/general/fact_Enrollment', primary_key='EnrollmentId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.) Build fact_MeetingAttendance Table\r\n",
        "\r\n",
        "Data aggregations curation on Insights activity data and Graph meeting_attendance_report pertaining to meetings (as well as using a few other tables).\r\n",
        "\r\n",
        "This table has one row per student per meeting recorded from in both Insights and from the Graph query, with details around:\r\n",
        " - meeting attendance ID (unique ID per student per meeting - scraped from Insights Activity SignalId),\r\n",
        " - school ID (ID from Organization table),\r\n",
        " - course ID, \r\n",
        " - section ID, \r\n",
        " - instructor ID, \r\n",
        " - student ID (from Person table),\r\n",
        " - meeting ID (unique ID per meeting - same across Insights activity and Graph meeting query),\r\n",
        " - student join time, \r\n",
        " - student leave time,\r\n",
        " - student attendance time in seconds.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/fact_MeetingAttendance```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:55:59.3964133Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:55:59.5133054Z",
              "execution_finish_time": "2023-01-11T18:56:02.355334Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfInsights_aadgroup = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadGroup')\r\n",
        "dfInsights_aaduserpersonmap = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadUserPersonMapping')\r\n",
        "dfInsights_activity = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/activity')\r\n",
        "dfInsights_enrollment = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/Enrollment')\r\n",
        "dfGraph_meetingAtten = oea.load('stage2/Refined/graph_api/v1.0/general/meeting_attendance_report')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aadgroup_np = oea.load('stage2/Refined/M365/v'+ insights_version +'/sensitive/AadGroup_lookup')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:56:00.668086Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:56:02.5344037Z",
              "execution_finish_time": "2023-01-11T18:56:08.4903376Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "d4a37f2f-6f30-4d10-9e18-94bfd6802c11",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, d4a37f2f-6f30-4d10-9e18-94bfd6802c11)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# grab only the relevant columns from the Graph meeting attendance report data, then join with the Insights AADUserPersonMapping to join with Insights activity data\r\n",
        "dfMeetingAtten = dfGraph_meetingAtten.filter(dfGraph_meetingAtten['role'] == 'Student').select('userId_pseudonym', 'meetingId', 'attendanceInterval_joinDateTime', 'attendanceInterval_leaveDateTime', 'totalAttendanceInSec')\r\n",
        "dfMeetingAtten = dfMeetingAtten.withColumnRenamed('userId_pseudonym', 'StudentId_pseudonym').withColumnRenamed('meetingId', 'MeetingId').withColumnRenamed('attendanceInterval_joinDateTime', 'JoinTime') \\\r\n",
        "                                .withColumnRenamed('attendanceInterval_leaveDateTime', 'LeaveTime').withColumnRenamed('totalAttendanceInSec', 'AttendanceTime_sec')\r\n",
        "df_metadata = dfInsights_aaduserpersonmap.select('ObjectId_pseudonym', 'PersonId_pseudonym')\r\n",
        "dfMeetingAtten = dfMeetingAtten.join(df_metadata, dfMeetingAtten.StudentId_pseudonym == df_metadata.PersonId_pseudonym, how='inner').drop('PersonId_pseudonym')\r\n",
        "# then join with Insights activity data to isolate meeting data\r\n",
        "df_metadata = dfInsights_activity.filter(dfInsights_activity['AppName'] == 'TeamsMeeting').filter(dfInsights_activity['ActorRole'] == 'Student')\r\n",
        "df_metadata = df_metadata.select('SignalId', 'ClassId', 'ActorId_pseudonym', 'MeetingSessionId')\r\n",
        "dfMeetingAtten = dfMeetingAtten.join(df_metadata, (dfMeetingAtten.MeetingId == df_metadata.MeetingSessionId) & (dfMeetingAtten.ObjectId_pseudonym == df_metadata.ActorId_pseudonym), how='inner') \\\r\n",
        "                            .drop('ActorId_pseudonym', 'MeetingSessionId', 'ObjectId_pseudonym').withColumnRenamed('SignalId', 'MeetingAttendanceId')\r\n",
        "# join the Insights AADGroup_pseudo and _lookup table to the dfMeetingAtten table, to provide mapping of the hashed and non-hashed AADGroup Class IDs\r\n",
        "dfInsights_aadgroup_np_ = dfInsights_aadgroup_np.select('ObjectId', 'ObjectId_pseudonym').withColumnRenamed('ObjectId', 'Id').withColumnRenamed('ObjectId_pseudonym', 'AADGroup_ClassId_pseudonym')\r\n",
        "dfMeetingAtten = dfMeetingAtten.join(dfInsights_aadgroup_np_, dfMeetingAtten.ClassId == dfInsights_aadgroup_np_.Id, how='inner').drop('Id')\r\n",
        "dfInsights_aadgroup_ = dfInsights_aadgroup.select('ObjectId_pseudonym', 'SectionId')\r\n",
        "dfMeetingAtten = dfMeetingAtten.join(dfInsights_aadgroup_, dfMeetingAtten.AADGroup_ClassId_pseudonym == dfInsights_aadgroup_.ObjectId_pseudonym, how='inner') \\\r\n",
        "                            .drop('ClassId', 'AADGroup_ClassId_pseudonym', 'ObjectId_pseudonym')\r\n",
        "# finally, join dfMeetingAtten with dfEnroll table to grab the school, course and instructor IDs\r\n",
        "df_metadata = dfEnroll.select('SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym')\r\n",
        "df_metadata = df_metadata.withColumnRenamed('SectionId', 'classId').withColumnRenamed('StudentId_pseudonym', 'id')\r\n",
        "dfMeetingAtten = dfMeetingAtten.join(df_metadata, (dfMeetingAtten.StudentId_pseudonym == df_metadata.id) & (dfMeetingAtten.SectionId == df_metadata.classId), how='inner').drop('classId', 'id')\r\n",
        "dfMeetingAtten = dfMeetingAtten.select('MeetingAttendanceId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'MeetingId', 'JoinTime', 'LeaveTime', 'AttendanceTime_sec')\r\n",
        "display(dfMeetingAtten.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write to Stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:57:13.2561715Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:57:13.3755186Z",
              "execution_finish_time": "2023-01-11T18:57:26.1092577Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 18:57:25,115 - OEA - INFO - Number of new inbound rows processed: 10170\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "10170"
          },
          "execution_count": 23,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "publish(dfMeetingAtten, 'stage2/Enriched/learning_analytics/v1.0/general/fact_MeetingAttendance', 'stage3/Published/learning_analytics/v1.0/general/fact_MeetingAttendance', primary_key='MeetingAttendanceId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.) Build fact_Assignment Table\r\n",
        "\r\n",
        "Data aggregations curation on Insights activity data pertaining to assignments in the education system (as well as using a few other tables).\r\n",
        "\r\n",
        "This table has one row per student per assignment from the Insights activity data, with details around:\r\n",
        " - assignment ID (unique ID per assignment - scraped from Insights Activity AssignmentId),\r\n",
        " - school ID (ID from Organization table),\r\n",
        " - course ID, \r\n",
        " - section ID, \r\n",
        " - instructor ID, \r\n",
        " - student ID (from Person table),\r\n",
        " - assignment status ID (ID from dim_AssignmentStatus for student assignment status),\r\n",
        " - assigned date, \r\n",
        " - assignment due date, and\r\n",
        " - student grade.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/fact_Assignment```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:57:40.7233398Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:57:40.8635417Z",
              "execution_finish_time": "2023-01-11T18:57:41.4211408Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfInsights_aadgroup = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadGroup')\r\n",
        "dfInsights_aaduserpersonmap = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadUserPersonMapping')\r\n",
        "dfInsights_activity = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/activity')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aadgroup_np = oea.load('stage2/Refined/M365/v'+ insights_version +'/sensitive/AadGroup_lookup')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:32:35.9006164Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:32:36.063162Z",
              "execution_finish_time": "2023-01-11T19:32:40.4153369Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "22381c9b-6dd0-4dcd-bc8c-c085633cdb44",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 22381c9b-6dd0-4dcd-bc8c-c085633cdb44)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# NOTE: clean up below\r\n",
        "# grab only the relevant columns from the Insights activity student assignment data\r\n",
        "dfAssign = dfInsights_activity.filter(dfInsights_activity['AppName'] == 'Assignments').filter(dfInsights_activity['ActorRole'] == 'Student')\r\n",
        "dfAssign = dfAssign.select('StartTime', 'ClassId', 'ActorId_pseudonym', 'AssignmentId', 'Action', 'DueDate', 'Grade').withColumnRenamed('StartTime', 'AssignmentStatusTime')\r\n",
        "# join the Insights AADGroup_pseudo and _lookup table to the dfAssign table, to provide mapping of the hashed and non-hashed AADGroup Class IDs\r\n",
        "dfInsights_aadgroup_np_ = dfInsights_aadgroup_np.select('ObjectId', 'ObjectId_pseudonym').withColumnRenamed('ObjectId', 'Id').withColumnRenamed('ObjectId_pseudonym', 'AADGroup_ClassId_pseudonym')\r\n",
        "dfAssign = dfAssign.join(dfInsights_aadgroup_np_, dfAssign.ClassId == dfInsights_aadgroup_np_.Id, how='inner').drop('Id')\r\n",
        "dfInsights_aadgroup_ = dfInsights_aadgroup.select('ObjectId_pseudonym', 'SectionId')\r\n",
        "dfAssign = dfAssign.join(dfInsights_aadgroup_, dfAssign.AADGroup_ClassId_pseudonym == dfInsights_aadgroup_.ObjectId_pseudonym, how='inner') \\\r\n",
        "                            .drop('ClassId', 'AADGroup_ClassId_pseudonym', 'ObjectId_pseudonym')\r\n",
        "# join the Insights AADUserPersonMapping table to get the student IDs\r\n",
        "df_metadata = dfInsights_aaduserpersonmap.select('ObjectId_pseudonym', 'PersonId_pseudonym')\r\n",
        "dfAssign = dfAssign.join(df_metadata, dfAssign.ActorId_pseudonym == df_metadata.ObjectId_pseudonym, how='inner').drop('ActorId_pseudonym', 'ObjectId_pseudonym')\r\n",
        "dfAssign = dfAssign.withColumnRenamed('PersonId_pseudonym', 'StudentId_pseudonym')\r\n",
        "# scrape the Insights activity data for when each Instructor assigned the assignment\r\n",
        "df_metadata = dfInsights_activity.filter(dfInsights_activity['SignalType'] == 'AssignmentEvent').filter(dfInsights_activity['Action'] == 'Assigned')\r\n",
        "df_metadata = df_metadata.select('StartTime', 'AssignmentId').withColumn('AssignedDate', F.to_date(F.col('StartTime')))\r\n",
        "df_metadata = df_metadata.withColumnRenamed('AssignmentId', 'id').drop('StartTime')\r\n",
        "dfAssign = dfAssign.join(df_metadata, dfAssign.AssignmentId == df_metadata.id, how='left').drop('id')\r\n",
        "# assign a status ID to each student assignment status update\r\n",
        "dfAssign = dfAssign.withColumn('AssignmentStatusId', F.when(F.col('Action') == 'Assigned', '1').otherwise(F.when(F.col('Action') == 'Visited', '2').otherwise(F.when(F.col('Action') == 'Submitted', '3').otherwise(F.when(F.col('Action') == 'Returned', '4')))))\r\n",
        "dfAssign = dfAssign.drop('Action')\r\n",
        "# join dfAssign with dfEnroll table to grab the school, course and instructor IDs\r\n",
        "df_metadata = dfEnroll.select('SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym')\r\n",
        "df_metadata = df_metadata.withColumnRenamed('SectionId', 'classId').withColumnRenamed('StudentId_pseudonym', 'id')\r\n",
        "dfAssign = dfAssign.join(df_metadata, (dfAssign.StudentId_pseudonym == df_metadata.id) & (dfAssign.SectionId == df_metadata.classId), how='inner').drop('classId', 'id')\r\n",
        "dfAssign = dfAssign.select('AssignmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'AssignmentStatusId', 'AssignmentStatusTime', 'AssignedDate', 'DueDate', 'Grade')\r\n",
        "# finally, inject every instance of an assignment being assigned (Insights activity data doesn't originally assign each assignment to each student)\r\n",
        "df_metadata = dfAssign.groupBy('AssignmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'AssignedDate', 'DueDate').count()\r\n",
        "df_metadata = df_metadata.drop('count')\r\n",
        "df_metadata = df_metadata.withColumn('AssignmentStatusTime', F.col('AssignedDate')).withColumn('AssignmentStatusTime', F.col('AssignmentStatusTime').cast(TimestampType()))\r\n",
        "df_metadata = df_metadata.withColumn('AssignmentStatusId', F.lit('1')).withColumn('Grade', F.lit(None))\r\n",
        "df_metadata = df_metadata.select('AssignmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'AssignmentStatusId', 'AssignmentStatusTime', 'AssignedDate', 'DueDate', 'Grade')\r\n",
        "dfAssign = dfAssign.unionAll(df_metadata)\r\n",
        "# drop the AssignedDate and DueDate columns, and separate the AssignmentStatusTime column into AssignmentStatusDate and AssignmentStatusTime\r\n",
        "dfAssign = dfAssign.drop('AssignedDate', 'DueDate').withColumn('AssignmentStatusDate', F.to_date(F.col('AssignmentStatusTime'))).withColumn('AssignmentStatusTime', F.date_format('AssignmentStatusTime', 'HH:mm:ss'))\r\n",
        "dfAssign = dfAssign.select('AssignmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'AssignmentStatusId', 'AssignmentStatusDate', 'AssignmentStatusTime', 'Grade')\r\n",
        "display(dfAssign.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ad hoc - test data doesn't have grades rounded, so this rounds the grade to the nearest 2 decimal places.\r\n",
        "dfAssign = dfAssign.withColumn('Grade', F.round(F.col('Grade'), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:33:16.1736803Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:33:16.3196418Z",
              "execution_finish_time": "2023-01-11T19:33:20.3232654Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "436cf651-8042-4d9f-92db-6ab780ef9d02",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 436cf651-8042-4d9f-92db-6ab780ef9d02)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# create a unique primary key, since publishing will drop duplicates, and we intentionally want Assignment Id's reoccuring\r\n",
        "dfAssign = dfAssign.withColumn('AssignmentActivityId', F.concat(F.col('AssignmentStatusTime'),F.lit('_'),F.col('StudentId_pseudonym'),F.lit('_'),F.col('AssignmentId')))\r\n",
        "dfAssign = dfAssign.select('AssignmentActivityId', 'AssignmentId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'AssignmentStatusId', 'AssignmentStatusDate', 'AssignmentStatusTime', 'Grade')\r\n",
        "\r\n",
        "display(dfAssign.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write to Stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:34:53.7562541Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:34:53.9360935Z",
              "execution_finish_time": "2023-01-11T19:35:07.7101457Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 19:35:06,814 - OEA - INFO - Number of new inbound rows processed: 30785\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "30785"
          },
          "execution_count": 53,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "publish(dfAssign, 'stage2/Enriched/learning_analytics/v1.0/general/fact_Assignment', 'stage3/Published/learning_analytics/v1.0/general/fact_Assignment', primary_key='AssignmentActivityId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4.) Build fact_Activity Table\r\n",
        "\r\n",
        "Data aggregations curation on Insights activity data in the education system (as well as using a few other tables).\r\n",
        "\r\n",
        "This table has one row per student per signal from the Insights activity data, with details around:\r\n",
        " - activity ID (unique ID per assignment - scraped from Insights Activity SignalId),\r\n",
        " - school ID (ID from Organization table),\r\n",
        " - course ID, \r\n",
        " - section ID, \r\n",
        " - instructor ID, \r\n",
        " - student ID (from Person table),\r\n",
        " - signal type ID (ID from dim_SignalType for student signal type),\r\n",
        " - activity date (i.e. date of the Insights activity signal), and\r\n",
        " - activity start time (i.e. datetime of the Insights activity signal).\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/fact_Activity```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:35:14.1484923Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:35:14.2586097Z",
              "execution_finish_time": "2023-01-11T19:35:14.9823978Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 26, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfInsights_aadgroup = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadGroup')\r\n",
        "dfInsights_aaduserpersonmap = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/AadUserPersonMapping')\r\n",
        "dfInsights_activity = oea.load('stage2/Refined/M365/v'+ insights_version +'/general/activity')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aadgroup_np = oea.load('stage2/Refined/M365/v'+ insights_version +'/sensitive/AadGroup_lookup')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:35:17.604543Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:35:17.7176684Z",
              "execution_finish_time": "2023-01-11T19:35:18.2523539Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 27, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# NOTE: clean up below\r\n",
        "# join IDs with Person IDs rather than AADUser IDs from Insights activity data\r\n",
        "dfActivity = dfInsights_activity.select('SignalType', 'StartTime', 'SignalId', 'ClassId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "df_metadata = dfInsights_aaduserpersonmap.select('ObjectId_pseudonym', 'PersonId_pseudonym')\r\n",
        "dfActivity = dfActivity.join(df_metadata, dfActivity.ActorId_pseudonym == df_metadata.ObjectId_pseudonym, how='left').drop('ActorId_pseudonym', 'ObjectId_pseudonym').withColumnRenamed('PersonId_pseudonym', 'ActorId_pseudonym')\r\n",
        "# separate Instructor IDs (via Actor IDs)\r\n",
        "df_metadata = dfActivity.select('SignalId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['ActorRole'] != 'Student')\r\n",
        "df_metadata = df_metadata.drop('ActorRole').withColumnRenamed('ActorId_pseudonym', 'InstructorId_pseudonym').withColumnRenamed('SignalId', 'id')\r\n",
        "dfActivity = dfActivity.join(df_metadata, dfActivity.SignalId == df_metadata.id, how='left')\r\n",
        "dfActivity = dfActivity.drop('id')\r\n",
        "# seperate Student IDs (via Actor IDs)\r\n",
        "df_metadata = dfActivity.select('SignalId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['ActorRole'] == 'Student')\r\n",
        "df_metadata = df_metadata.drop('ActorRole').withColumnRenamed('ActorId_pseudonym', 'StudentId_pseudonym').withColumnRenamed('SignalId', 'id')\r\n",
        "dfActivity = dfActivity.join(df_metadata, dfActivity.SignalId == df_metadata.id, how='left')\r\n",
        "dfActivity = dfActivity.drop('id', 'ActorId_pseudonym')\r\n",
        "dfActivity = dfActivity.select('SignalId', 'SignalType', 'StartTime', 'ClassId', 'StudentId_pseudonym', 'InstructorId_pseudonym')\r\n",
        "# replace AADGroup IDs with Section IDs\r\n",
        "dfInsights_aadgroup_np_ = dfInsights_aadgroup_np.select('ObjectId', 'ObjectId_pseudonym').withColumnRenamed('ObjectId', 'Id').withColumnRenamed('ObjectId_pseudonym', 'AADGroup_ClassId_pseudonym')\r\n",
        "dfActivity = dfActivity.join(dfInsights_aadgroup_np_, dfActivity.ClassId == dfInsights_aadgroup_np_.Id, how='inner').drop('Id')\r\n",
        "dfInsights_aadgroup_ = dfInsights_aadgroup.select('ObjectId_pseudonym', 'SectionId')\r\n",
        "dfActivity = dfActivity.join(dfInsights_aadgroup_, dfActivity.AADGroup_ClassId_pseudonym == dfInsights_aadgroup_.ObjectId_pseudonym, how='inner') \\\r\n",
        "                            .drop('ClassId', 'AADGroup_ClassId_pseudonym', 'ObjectId_pseudonym')\r\n",
        "# clean data as needed and join dfActivity with dfEnroll table to grab the school, course and instructor IDs\r\n",
        "df_metadata = dfEnroll.select('SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym')\r\n",
        "df_metadata = df_metadata.withColumnRenamed('SectionId', 'classId').withColumnRenamed('StudentId_pseudonym', 'id').withColumnRenamed('InstructorId_pseudonym', 'id2')\r\n",
        "dfActivity = dfActivity.join(df_metadata, ((dfActivity.StudentId_pseudonym == df_metadata.id) | (dfActivity.InstructorId_pseudonym == df_metadata.id2)) & \\\r\n",
        "                            (dfActivity.SectionId == df_metadata.classId), how='inner').drop('classId', 'id', 'id2')\r\n",
        "dfActivity = dfActivity.withColumnRenamed('SignalId', 'ActivityId').withColumn('ActivityDate', F.to_date(F.col('StartTime')))\r\n",
        "dfActivity = dfActivity.select('ActivityId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'SignalType', 'ActivityDate', 'StartTime')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:35:21.6025293Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:35:21.7035707Z",
              "execution_finish_time": "2023-01-11T19:35:28.621659Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 28, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "429ac374-d171-4ab2-90b5-9b044a9e8a92",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 429ac374-d171-4ab2-90b5-9b044a9e8a92)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# NOTE: Clean up SignalType assignment/encoding process.\r\n",
        "def SignalTypeId(SignalType):\r\n",
        "    if SignalType == 'PostChannelMessage':\r\n",
        "        res = '000001'\r\n",
        "    elif SignalType == 'ReplyChannelMessage':\r\n",
        "        res = '000002'\r\n",
        "    elif SignalType == 'VisitTeamChannel':\r\n",
        "        res = '000003'\r\n",
        "    elif SignalType == 'ExpandChannelMessage':\r\n",
        "        res = '000004'\r\n",
        "    elif SignalType == 'ReactedWithEmoji':\r\n",
        "        res = '000005'\r\n",
        "    elif SignalType == 'Like':\r\n",
        "        res = '000010'\r\n",
        "    elif SignalType == 'Unlike':\r\n",
        "        res = '000011'\r\n",
        "    elif SignalType == 'FileAccessed':\r\n",
        "        res = '000020'\r\n",
        "    elif SignalType == 'FileModified':\r\n",
        "        res = '000021'\r\n",
        "    elif SignalType == 'FileDownloaded':\r\n",
        "        res = '000022'\r\n",
        "    elif SignalType == 'FileUploaded':\r\n",
        "        res = '000023'\r\n",
        "    elif SignalType == 'ShareNotificationRequested':\r\n",
        "        res = '000030'\r\n",
        "    elif SignalType == 'AddedToSharedWithMe':\r\n",
        "        res = '000040'\r\n",
        "    elif SignalType == 'CommentCreated':\r\n",
        "        res = '000050'\r\n",
        "    elif SignalType == 'CommentDeleted':\r\n",
        "        res = '000051'\r\n",
        "    elif SignalType == 'UserAtMentioned':\r\n",
        "        res = '000060'\r\n",
        "    elif SignalType == 'Reflect':\r\n",
        "        res = '000100'\r\n",
        "    elif SignalType == 'OneNotePageChanged':\r\n",
        "        res = '001000'\r\n",
        "    elif SignalType == 'SubmissionEvent':\r\n",
        "        res = '020000'\r\n",
        "    elif SignalType == 'AssignmentEvent':\r\n",
        "        res = '010000'\r\n",
        "    elif SignalType == 'CallRecordSummarized':\r\n",
        "        res = '100000'\r\n",
        "    else:\r\n",
        "        res = ''\r\n",
        "    return res\r\n",
        "# define the function/dataType\r\n",
        "new_f2 = F.udf(SignalTypeId, StringType())\r\n",
        "  \r\n",
        "# add the new column\r\n",
        "dfActivity = dfActivity.withColumn('SignalTypeId', new_f2('SignalType'))\r\n",
        "dfActivity = dfActivity.select('ActivityId', 'SchoolId', 'CourseId', 'SectionId', 'InstructorId_pseudonym', 'StudentId_pseudonym', 'SignalTypeId', 'ActivityDate', 'StartTime')\r\n",
        "display(dfActivity.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write to Stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "59",
              "statement_id": 29,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T19:36:31.8736581Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T19:36:31.9835672Z",
              "execution_finish_time": "2023-01-11T19:36:44.7770869Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 59, 29, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-11 19:36:42,713 - OEA - INFO - Number of new inbound rows processed: 34379\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "34379"
          },
          "execution_count": 61,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "publish(dfActivity, 'stage2/Enriched/learning_analytics/v1.0/general/fact_Activity', 'stage3/Published/learning_analytics/v1.0/general/fact_Activity', primary_key='ActivityId')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}