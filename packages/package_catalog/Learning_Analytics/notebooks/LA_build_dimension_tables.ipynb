{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Analytics Package: Build Dimension Tables\r\n",
        "\r\n",
        "Builds the dimension tables used for the Learning Analytics package **v1.0** dashboard, in the context of using the Higher Ed. test data from Microsoft Education Insights roster, activity, and Microsoft Graph meeting attendance data.\r\n",
        "\r\n",
        "The following tables are created in each of the steps outlined below (and above the cells for enrichment/curation):\r\n",
        "\r\n",
        "1. dim_Student,\r\n",
        "2. dim_Student_lookup,\r\n",
        "3. dim_Instructor,\r\n",
        "4. dim_Section,\r\n",
        "5. dim_Course,\r\n",
        "6. dim_School,\r\n",
        "7. dim_Meeting,\r\n",
        "8. dim_AssignmentStatus,\r\n",
        "9. dim_Assignment\r\n",
        "10. dim_SignalType, and\r\n",
        "11. dim_Date.\r\n",
        "\r\n",
        "This package-notebook also uses two methods (defined and outlined below):\r\n",
        " - **_publish_to_stage2**: uses the OEA_py function *upsert* to land the current dataframe in stage2/Enriched\r\n",
        " - **publish**: uses the method above to land the package dimension tables in stage2, then writes the same table to stage3/Published. Delta checkpoints are landed in the respective stage2/Enriched directory."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'\r\n",
        "insights_version = '1.14'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "97",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-05-22T02:58:44.774851Z",
              "session_start_time": "2023-05-22T02:58:44.862967Z",
              "execution_start_time": "2023-05-22T02:58:44.9947431Z",
              "execution_finish_time": "2023-05-22T02:58:45.1734888Z",
              "spark_jobs": null,
              "parent_msg_id": "67cf7dd1-8a13-4360-a5f6-efa168ac7496"
            },
            "text/plain": "StatementMeta(spark3p3sm, 97, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "97",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-05-22T02:58:46.9408491Z",
              "session_start_time": null,
              "execution_start_time": "2023-05-22T02:58:56.2767959Z",
              "execution_finish_time": "2023-05-22T02:58:56.2769931Z",
              "spark_jobs": null,
              "parent_msg_id": "46871675-7513-4363-90de-1894d3876623"
            },
            "text/plain": "StatementMeta(, 97, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-22 02:58:54,806 - OEA - INFO - Now using workspace: dev\n2023-05-22 02:58:54,807 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "97",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-05-22T03:00:06.155075Z",
              "session_start_time": null,
              "execution_start_time": "2023-05-22T03:00:06.3075439Z",
              "execution_finish_time": "2023-05-22T03:00:06.4841675Z",
              "spark_jobs": null,
              "parent_msg_id": "884a0177-4f78-499c-ad15-122817763feb"
            },
            "text/plain": "StatementMeta(spark3p3sm, 97, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-22 03:00:06,285 - OEA - INFO - Now using workspace: dev\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.) Define the Publish Function\r\n",
        "\r\n",
        "### Notes\r\n",
        " - Can go about this in 2 ways (*thinking the second makes more sense*): \r\n",
        "   1. Write a new function (publish) that reads in a set of dfs, the aggregations, etc. writes checkpoints out to a new folder in stage 2, and then writes updated schema to stage3\r\n",
        "   2. Perform aggregations and write finalized table to stage2, then publish to stage3 (keeps checkpoints in stage2 folder).\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _publish_to_stage2(df, destination, pk):\r\n",
        "    oea.upsert(df, destination, pk)\r\n",
        "\r\n",
        "def publish(df, stage2_destination, stage3_destination, primary_key='id'):\r\n",
        "    _publish_to_stage2(df, stage2_destination, primary_key)\r\n",
        "\r\n",
        "    spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "    streaming_df = spark.readStream.format('delta').load(oea.to_url(stage2_destination))\r\n",
        "    # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\r\n",
        "    query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(stage2_destination) + '/_checkpoints')\r\n",
        "    query = query.start(oea.to_url(stage3_destination))\r\n",
        "    query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "    number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\r\n",
        "    logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\r\n",
        "    logger.debug(query.lastProgress)\r\n",
        "    return number_of_new_inbound_rows"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "97",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-05-22T03:00:23.6275596Z",
              "session_start_time": null,
              "execution_start_time": "2023-05-22T03:00:23.8009497Z",
              "execution_finish_time": "2023-05-22T03:00:23.9806016Z",
              "spark_jobs": null,
              "parent_msg_id": "3d083879-164b-43a4-ade8-9975f94f7f25"
            },
            "text/plain": "StatementMeta(spark3p3sm, 97, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.) Build dim_Student Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights roster data: AADUser and Person (as well as others).\r\n",
        "\r\n",
        "This table has one row per student in the Insights roster data for the education system, with details around:\r\n",
        " - student IDs (previously, student internal ID or PersonId from Person table), \r\n",
        " - student names (masked), \r\n",
        " - student educational-grade levels, \r\n",
        " - student UserPrincipalNames (pseudonymized UPNs from the AADUser table),\r\n",
        " - student date of birth (from PersonDemographic table, currently hashed here), and\r\n",
        " - student address (currently, birth city and state - which is hashed for pseudo data - since Insights data doesn't hold current address data).\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Student```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in tables needed\r\n",
        "dfInsights_aaduserpersonmapping = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadUserPersonMapping')\r\n",
        "dfInsights_aaduser = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadUser')\r\n",
        "dfInsights_person = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Person')\r\n",
        "dfInsights_personOrgRole = oea.load('stage2/Refined/M365/v' + insights_version + '/general/PersonOrganizationRole')\r\n",
        "dfInsights_refDefinition = oea.load('stage2/Refined/M365/v' + insights_version + '/general/RefDefinition')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_persondemographic_np = oea.load('stage2/Refined/M365/v' + insights_version + '/sensitive/PersonDemographic_lookup')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T17:56:52.4154089Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:00:05.6676706Z",
              "execution_finish_time": "2023-01-11T18:00:57.8995149Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idea is eventually to actually write the OEA standardized schema to stage2/Enriched, then perform the aggregations and cleaning, and then write to stage3/Published.\r\n",
        "# NOTE: Update as needed; remove unused or irrelelvant columns.\r\n",
        "\r\n",
        "# start with Insights roster tables: Person and join with PersonOrganizationRole - to create the start of the Student table\r\n",
        "dfInsights = dfInsights_personOrgRole.join(dfInsights_person, dfInsights_personOrgRole.PersonId_pseudonym == dfInsights_person.Id_pseudonym, how='inner')\r\n",
        "dfInsights = dfInsights.select('PersonId_pseudonym', 'Surname', 'GivenName', 'MiddleName', 'RefRoleId', 'RefGradeLevelId')\r\n",
        "# join Insights table with the RefDefinition table - to get the Role of each person, and filter by only student SIS data\r\n",
        "dfInsights = dfInsights.join(dfInsights_refDefinition, dfInsights.RefRoleId == dfInsights_refDefinition.Id, how='inner')\r\n",
        "dfInsights = dfInsights.withColumnRenamed('Code', 'PersonRole')\r\n",
        "dfStudent = dfInsights.filter(dfInsights['PersonRole'] == 'Student')\r\n",
        "dfStudent = dfStudent.select('PersonId_pseudonym', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'RefGradeLevelId')\r\n",
        "# join Student table with RefDefinition table, again - to get the grade of each student\r\n",
        "dfStudent = dfStudent.join(dfInsights_refDefinition, dfInsights.RefGradeLevelId == dfInsights_refDefinition.Id, how='inner')\r\n",
        "dfStudent = dfStudent.withColumnRenamed('Code', 'StudentGrade')\r\n",
        "dfStudent = dfStudent.select('PersonId_pseudonym', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade')\r\n",
        "# join Student table with the AADUserPersonMapping table - to get the External Student ID\r\n",
        "dfInsights_aaduserpersonmapping = dfInsights_aaduserpersonmapping.withColumnRenamed('PersonId_pseudonym', 'StudentId_internal_pseudonym')\r\n",
        "dfStudent = dfStudent.join(dfInsights_aaduserpersonmapping, dfStudent.PersonId_pseudonym == dfInsights_aaduserpersonmapping.StudentId_internal_pseudonym, how='inner')\r\n",
        "dfStudent = dfStudent.withColumnRenamed('ObjectId_pseudonym', 'StudentId_external_pseudonym')\r\n",
        "dfStudent = dfStudent.select('StudentId_internal_pseudonym', 'StudentId_external_pseudonym', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade')\r\n",
        "# join Student table with AAD User table to extract each student's UPN, and remove unneeded columns \r\n",
        "df_metadata = dfInsights_aaduser.select('ObjectId_pseudonym', 'UserPrincipalName_pseudonym')\r\n",
        "dfStudent = dfStudent.join(df_metadata, dfStudent.StudentId_external_pseudonym == df_metadata.ObjectId_pseudonym, how='inner')\r\n",
        "dfStudent = dfStudent.drop('ObjectId_pseudonym', 'PersonRole')\r\n",
        "# join with Person Demographic table to extract each student's DoB and birth location (since current address isn't available from Insights roster data)\r\n",
        "# also drop student external IDs (AAD IDs), and use Person ID as the primary key\r\n",
        "df_metadata = dfInsights_persondemographic_np.select('PersonId_pseudonym', 'BirthDate', 'BirthCity', 'BirthState')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['BirthDate'] != 'BirthDate')\r\n",
        "dfStudent = dfStudent.join(df_metadata, dfStudent.StudentId_internal_pseudonym == df_metadata.PersonId_pseudonym, how='inner')\r\n",
        "dfStudent = dfStudent.drop('PersonId_pseudonym', 'StudentId_external_pseudonym').withColumnRenamed('StudentId_internal_pseudonym', 'StudentId_pseudonym')\r\n",
        "display(dfStudent.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T17:57:26.0547116Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:00:58.0250363Z",
              "execution_finish_time": "2023-01-11T18:01:06.7142121Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "389813b4-c8d4-4382-bdea-a2690818eedc",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 389813b4-c8d4-4382-bdea-a2690818eedc)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfStudent, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Student', 'stage3/Published/learning_analytics/v1.0/general/dim_Student', primary_key='StudentId_pseudonym')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:09:57.7413184Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:09:57.8476268Z",
              "execution_finish_time": "2023-01-11T18:10:06.517043Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:10:06,073 - OEA - INFO - Number of new inbound rows processed: 600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "600"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.) Build dim_Student_lookup Table\r\n",
        "\r\n",
        "Data aggregations and curation on previously created dim_Student table and Insights roster data: Person_lookup (as well as others).\r\n",
        "\r\n",
        "This table has one row per student in the Insights roster data, with details around:\r\n",
        " - student IDs (previously, student internal ID or PersonId from Person table; contains both hashed-pseudonym IDs and unhashed IDs), \r\n",
        " - student UserPrincipalNames (contains both hashed-pseudonym UPNs and unhashed UPNs), and\r\n",
        " - student names (unmasked).\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/sensitive/dim_Student_lookup```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_person_np = oea.load('stage2/Refined/M365/v'+ insights_version +'/sensitive/Person_lookup')\r\n",
        "dfInsights_aaduser_np = oea.load('stage2/Refined/M365/v' + insights_version + '/sensitive/AadUser_lookup')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:11:10.8750626Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:11:11.0306528Z",
              "execution_finish_time": "2023-01-11T18:11:12.8972788Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isolate relevant fields from dfStudent, and join with Person_lookup to get unhashed IDs as well as student names\r\n",
        "dfStudent_lookup = dfStudent.select('StudentId_pseudonym', 'UserPrincipalName_pseudonym')\r\n",
        "dfStudent_lookup = dfStudent_lookup.join(dfInsights_person_np, dfStudent_lookup.StudentId_pseudonym == dfInsights_person_np.Id_pseudonym, how='inner').drop('Id_pseudonym')\r\n",
        "dfStudent_lookup = dfStudent_lookup.select('StudentId_pseudonym','Id','Surname','GivenName','MiddleName','UserPrincipalName_pseudonym').withColumnRenamed('Id', 'StudentId')\r\n",
        "# join dfStudent with AADUser_lookup unhashed UPNs\r\n",
        "dfInsights_aaduser_np_ = dfInsights_aaduser_np.select('UserPrincipalName', 'UserPrincipalName_pseudonym').withColumnRenamed('UserPrincipalName_pseudonym', 'UPN_pseudo')\r\n",
        "dfStudent_lookup = dfStudent_lookup.join(dfInsights_aaduser_np_, dfStudent_lookup.UserPrincipalName_pseudonym == dfInsights_aaduser_np_.UPN_pseudo, how='inner').drop('UPN_pseudo')\r\n",
        "display(dfStudent_lookup.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:11:20.3892032Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:11:20.4989114Z",
              "execution_finish_time": "2023-01-11T18:11:24.477678Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "9e25ee00-da5a-4a46-97de-0abfeab26a03",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 9e25ee00-da5a-4a46-97de-0abfeab26a03)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfStudent_lookup, 'stage2/Enriched/learning_analytics/v1.0/sensitive/dim_Student_lookup', 'stage3/Published/learning_analytics/v1.0/sensitive/dim_Student_lookup', primary_key='StudentId_pseudonym')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:12:47.6838757Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:12:47.8073835Z",
              "execution_finish_time": "2023-01-11T18:12:58.4520252Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:12:57,409 - OEA - INFO - Number of new inbound rows processed: 600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "600"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.) Build dim_Instructor Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights activity and roster data: activity and Person (as well as others). *Note*: This only grabs the instructors listed in the Insights activity data; update for production purposes.\r\n",
        "\r\n",
        "This table has one row per instructor in the Insights roster data for the education system, with details around:\r\n",
        " - instructor IDs (previously, instructor internal ID or Person ID from the Person table), and\r\n",
        " - instructor names.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Instructor```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')\r\n",
        "dfInsights_aaduser = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadUser')\r\n",
        "dfInsights_aaduserpersonmapping = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadUserPersonMapping')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aaduser_np = oea.load('stage2/Refined/M365/v' + insights_version + '/sensitive/AadUser_lookup')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:13:11.4151254Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:13:11.5525719Z",
              "execution_finish_time": "2023-01-11T18:13:13.4143521Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 18, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Clean up this to only do the processes needed\r\n",
        "\"\"\"CONSIDER CREATING INSTRUCTOR GENERAL vs. LOOKUP\"\"\"\r\n",
        "# extract the classes/instructors initally provided from the Insights Activity test data, and extract the Instructor IDs, number of sections that the instructor currently teaches.\r\n",
        "dfInstructor = dfInsights_activity.filter(dfInsights_activity['ActorRole'] != 'Student')\r\n",
        "dfInstructor = dfInstructor.select('ClassId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "dfInstructor = dfInstructor.distinct()\r\n",
        "dfInstructor = dfInstructor.groupBy('ActorId_pseudonym').count()\r\n",
        "dfInstructor = dfInstructor.withColumnRenamed('ActorId_pseudonym', 'InstructorId_external_pseudonym').withColumnRenamed('count', 'NumSectionsCurrentlyInstructed')\r\n",
        "# use the AADUser_pseudo table to get the AAD User Object ID of each professor (to get their name)\r\n",
        "df_metadata = dfInsights_aaduser.filter(dfInsights_aaduser['Role'] != 'student')\r\n",
        "df_metadata = df_metadata.select('ObjectId_pseudonym', 'TeacherId_pseudonym', 'Role')\r\n",
        "dfInstructor = dfInstructor.join(df_metadata, dfInstructor.InstructorId_external_pseudonym == df_metadata.TeacherId_pseudonym, how='left')\r\n",
        "dfInstructor = dfInstructor.drop('TeacherId_pseudonym').withColumnRenamed('Role', 'InstructorRole')\r\n",
        "# use the AADUser_lookup table to get each professor's name\r\n",
        "df_metadata2 = dfInsights_aaduser_np.select('UserPrincipalName', 'ObjectId_pseudonym').withColumnRenamed('ObjectId_pseudonym', 'id')\r\n",
        "dfInstructor = dfInstructor.join(df_metadata2, dfInstructor.ObjectId_pseudonym == df_metadata2.id, how='inner')\r\n",
        "dfInstructor = dfInstructor.drop('id').withColumnRenamed('UserPrincipalName', 'InstructorName')\r\n",
        "# add each instructor's hashed internal ID (i.e. ID from Insights Person table)\r\n",
        "df_metadata = dfInsights_aaduserpersonmapping.select('ObjectId_pseudonym', 'PersonId_pseudonym').withColumnRenamed('ObjectId_pseudonym', 'id')\r\n",
        "dfInstructor = dfInstructor.join(df_metadata, dfInstructor.ObjectId_pseudonym == df_metadata.id, how='inner')\r\n",
        "dfInstructor = dfInstructor.withColumnRenamed('PersonId_pseudonym', 'InstructorId_internal_pseudonym')\r\n",
        "dfInstructor = dfInstructor.select('InstructorId_external_pseudonym', 'InstructorId_internal_pseudonym', 'InstructorRole', 'InstructorName', 'NumSectionsCurrentlyInstructed')\r\n",
        "display(dfInstructor.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:14:05.8744564Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:14:05.9933819Z",
              "execution_finish_time": "2023-01-11T18:14:11.4261022Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "b098aa1a-8ec1-4b5a-bc07-9b2afa12f79d",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, b098aa1a-8ec1-4b5a-bc07-9b2afa12f79d)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ad hoc - remove unused columns\r\n",
        "dfInstructor = dfInstructor.select('InstructorId_internal_pseudonym', 'InstructorName').withColumnRenamed('InstructorId_internal_pseudonym', 'InstructorId_pseudonym')\r\n",
        "display(dfInstructor.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:14:55.8178569Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:14:55.9334907Z",
              "execution_finish_time": "2023-01-11T18:14:57.784717Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "50982658-a53f-46c3-a681-b25810a23887",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 50982658-a53f-46c3-a681-b25810a23887)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfInstructor, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Instructor', 'stage3/Published/learning_analytics/v1.0/general/dim_Instructor', primary_key='InstructorId_pseudonym')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 21,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:16:01.0414767Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:16:01.1697754Z",
              "execution_finish_time": "2023-01-11T18:16:08.0725671Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 21, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:16:07,628 - OEA - INFO - Number of new inbound rows processed: 40\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "40"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.) Build dim_Section Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights roster data: Section and SectionSession (and others). \r\n",
        "\r\n",
        "This table has one row per section/class in the Insights Section table, with details around:\r\n",
        " - section ID, \r\n",
        " - section name,\r\n",
        " - section start & end date, and\r\n",
        " - calendar cycle that this particular section belongs to.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Section```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')\r\n",
        "dfInsights_aadgroup = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadGroup')\r\n",
        "#dfInsights_aadgroupmembership = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadGroupMembership')\r\n",
        "dfInsights_aaduserpersonmapping = oea.load('stage2/Refined/M365/v' + insights_version + '/general/AadUserPersonMapping')\r\n",
        "dfInsights_enrollment = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Enrollment')\r\n",
        "dfInsights_section = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Section')\r\n",
        "dfInsights_sectionsession = oea.load('stage2/Refined/M365/v' + insights_version + '/general/SectionSession')\r\n",
        "dfInsights_session = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Session')\r\n",
        "# lookup(s)\r\n",
        "dfInsights_aadgroup_np = oea.load('stage2/Refined/M365/v' + insights_version + '/sensitive/AadGroup_lookup')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:16:39.8833434Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:16:40.0119189Z",
              "execution_finish_time": "2023-01-11T18:16:45.3969437Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 22, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Clean up this to only do the processes needed\r\n",
        "# extract the classes initally provided from the Insights Activity test data, and extract the Teacher IDs \r\n",
        "dfClass = dfInsights_activity.filter(dfInsights_activity['ActorRole'] != 'Student')\r\n",
        "dfClass = dfClass.filter(dfClass['ActorRole'] != 'ActorRole')\r\n",
        "dfClass = dfClass.select('ClassId', 'ActorId_pseudonym', 'ActorRole')\r\n",
        "dfClass = dfClass.groupBy('ClassId', 'ActorId_pseudonym', 'ActorRole').count()\r\n",
        "dfClass = dfClass.withColumnRenamed('ClassId', 'AADGroup_ClassId').withColumnRenamed('ActorId_pseudonym', 'InstructorId_external_pseudonym').withColumnRenamed('ActorRole', 'PersonRole')\r\n",
        "dfClass = dfClass.drop('count')\r\n",
        "# join the Insights AADGroup_pseudo and _lookup table to the dfClass table, to provide mapping of the hashed and non-hashed AADGroup Class IDs\r\n",
        "dfInsights_aadgroup_np_ = dfInsights_aadgroup_np.select('ObjectId', 'DisplayName', 'ObjectId_pseudonym').withColumnRenamed('ObjectId', 'Id').withColumnRenamed('ObjectId_pseudonym', 'AADGroup_ClassId_pseudonym')\r\n",
        "dfClass = dfClass.join(dfInsights_aadgroup_np_, dfClass.AADGroup_ClassId == dfInsights_aadgroup_np_.Id, how='inner')\r\n",
        "dfClass = dfClass.drop('Id')\r\n",
        "dfInsights_aadgroup_ = dfInsights_aadgroup.select('ObjectId_pseudonym', 'SectionId')\r\n",
        "dfClass = dfClass.join(dfInsights_aadgroup_, dfClass.AADGroup_ClassId_pseudonym == dfInsights_aadgroup_.ObjectId_pseudonym, how='inner')\r\n",
        "dfClass = dfClass.drop('ObjectId_pseudonym', 'PersonRole').withColumnRenamed('DisplayName', 'SectionName')\r\n",
        "# then, use the Enrollment table to group by class/Section and count the number of students enrolled within each class\r\n",
        "df_metadata = dfInsights_enrollment.select('PersonId_pseudonym', 'SectionId', 'RefSectionRoleId').withColumnRenamed('SectionId', 'id')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['RefSectionRoleId'] == 'Student')\r\n",
        "df_metadata = df_metadata.groupBy('id').count()\r\n",
        "df_metadata = df_metadata.withColumnRenamed('count', 'NumStudentsEnrolledInSection')\r\n",
        "dfClass = dfClass.join(df_metadata, dfClass.SectionId == df_metadata.id, how='inner')\r\n",
        "dfClass = dfClass.select('SectionId', 'SectionName', 'InstructorId_external_pseudonym', 'NumStudentsEnrolledInSection')\r\n",
        "display(dfClass.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:16:56.4849025Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:16:56.6693912Z",
              "execution_finish_time": "2023-01-11T18:16:59.5920015Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "31bbb1f5-8bd1-4135-abc9-f81c87a083e6",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 31bbb1f5-8bd1-4135-abc9-f81c87a083e6)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ad hoc - drop now-unused columns and join with SectionSession data\r\n",
        "dfSection = dfClass.select('SectionId', 'SectionName')\r\n",
        "df_metadata1 = dfInsights_sectionsession.select('SectionId', 'SessionId').withColumnRenamed('SectionId', 'id')\r\n",
        "dfSection = dfSection.join(df_metadata1, dfSection.SectionId == df_metadata1.id, how='inner').drop('id')\r\n",
        "df_metadata2 = dfInsights_session.select('Id', 'Name', 'StartDate', 'EndDate')\r\n",
        "dfSection = dfSection.join(df_metadata2, dfSection.SessionId == df_metadata2.Id, how='inner').drop('SessionId', 'Id')\r\n",
        "dfSection = dfSection.select('SectionId', 'SectionName', 'StartDate', 'EndDate', 'Name').withColumnRenamed('StartDate', 'SectionStartDate').withColumnRenamed('EndDate', 'SectionEndDate').withColumnRenamed('Name', 'CalendarCycle')\r\n",
        "display(dfSection.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:17:05.1539985Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:17:05.2603049Z",
              "execution_finish_time": "2023-01-11T18:17:09.2041062Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "982736f4-7e65-4989-b373-d870ab4fc78e",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 982736f4-7e65-4989-b373-d870ab4fc78e)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfSection, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Section', 'stage3/Published/learning_analytics/v1.0/general/dim_Section', primary_key='SectionId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:18:30.7036186Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:18:30.8510335Z",
              "execution_finish_time": "2023-01-11T18:18:39.6738794Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:18:39,226 - OEA - INFO - Number of new inbound rows processed: 87\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "87"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.) Build dim_Course Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights roster data: Course and CourseGradeLevel (and others). \r\n",
        "\r\n",
        "This table has one row per course in the Insights Course table, with details around:\r\n",
        " - course ID, \r\n",
        " - course name,\r\n",
        " - course grade level, and\r\n",
        " - the number of students enrolled in each course.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Course```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_course = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Course')\r\n",
        "dfInsights_coursegradelevel = oea.load('stage2/Refined/M365/v' + insights_version + '/general/CourseGradeLevel')\r\n",
        "dfInsights_enrollment = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Enrollment')\r\n",
        "dfInsights_refdefinition = oea.load('stage2/Refined/M365/v' + insights_version + '/general/RefDefinition')\r\n",
        "dfInsights_section = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Section')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 29,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:30:33.7030433Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:30:33.8175526Z",
              "execution_finish_time": "2023-01-11T18:30:34.3431388Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 29, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Clean up this to only do the processes needed; CourseGradeLevel bug.\r\n",
        "# grab each course's ID and name, and RefGradeLevelId\r\n",
        "dfCourse = dfInsights_course.select('Id', 'Name')\r\n",
        "df_metadata = dfInsights_coursegradelevel.select('CourseId', 'RefGradeLevelId')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['CourseId'] != 'CourseId')\r\n",
        "dfCourse = dfCourse.join(df_metadata, dfCourse.Id == df_metadata.CourseId, how='inner').drop('CourseId')\r\n",
        "# join class table with RefDefinition table to get the grade level of each course\r\n",
        "df_metadata = dfInsights_refdefinition.select('Id', 'Code').withColumnRenamed('Id', 'refId')\r\n",
        "dfCourse = dfCourse.join(df_metadata, dfCourse.RefGradeLevelId == df_metadata.refId, how='inner').drop('refId', 'RefGradeLevelId')\r\n",
        "dfCourse = dfCourse.withColumnRenamed('Id', 'CourseId').withColumnRenamed('Name', 'CourseName').withColumnRenamed('Code', 'CourseGradeLevel')\r\n",
        "display(dfCourse.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:19:53.559018Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:19:53.6881386Z",
              "execution_finish_time": "2023-01-11T18:19:56.513875Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f02f70c3-f58f-4b11-b22c-e7fb58d51137",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, f02f70c3-f58f-4b11-b22c-e7fb58d51137)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This will need to be updated for production purposes.\r\n",
        "# add a column for the number of enrolled students - found through using the Insights Enrollment and Section tables\r\n",
        "df_metadata = dfInsights_enrollment.select('PersonId_pseudonym', 'SectionId', 'RefSectionRoleId')\r\n",
        "df_metadata = df_metadata.filter(df_metadata['RefSectionRoleId'] == 'Student')\r\n",
        "df_metadata2 = dfInsights_section.select('Id', 'CourseId')\r\n",
        "df_metadata = df_metadata.join(df_metadata2, df_metadata.SectionId == df_metadata2.Id, how='inner').drop('SectionId', 'Id')\r\n",
        "df_metadata = df_metadata.withColumnRenamed('CourseId', 'id')\r\n",
        "df_metadata = df_metadata.groupBy('id').count()\r\n",
        "df_metadata = df_metadata.withColumnRenamed('count', 'EnrolledStudents')\r\n",
        "dfCourse = dfCourse.join(df_metadata, dfCourse.CourseId == df_metadata.id, how='inner').drop('id')\r\n",
        "print('Number of Courses (should be 87):')\r\n",
        "print(dfCourse.count())\r\n",
        "display(dfCourse.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:30:56.0337517Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:30:56.1478908Z",
              "execution_finish_time": "2023-01-11T18:31:01.5134673Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 30, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Courses (should be 87):\n87\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "9dba11c3-5f29-4c8c-b699-c972ad858b07",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 9dba11c3-5f29-4c8c-b699-c972ad858b07)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfCourse, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Course', 'stage3/Published/learning_analytics/v1.0/general/dim_Course', primary_key='CourseId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:33:11.7672537Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:33:11.872578Z",
              "execution_finish_time": "2023-01-11T18:33:20.6358871Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:33:19,158 - OEA - INFO - Number of new inbound rows processed: 87\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 65,
          "data": {
            "text/plain": "87"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.) Build dim_School Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights roster data: Organization. \r\n",
        "\r\n",
        "This table has one row per school in the Insights Organization table, with details around:\r\n",
        " - school ID, \r\n",
        " - school name,\r\n",
        " - country of location, and\r\n",
        " - latitude & longitude coordinates of school location.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_School```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_org = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Organization')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 32,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:33:26.9802118Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:33:27.0870257Z",
              "execution_finish_time": "2023-01-11T18:33:28.2146896Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 32, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isolate the relevant columns from the Organization table\r\n",
        "dfSchool = dfInsights_org.select('Id', 'Name')\r\n",
        "dfSchool = dfSchool.withColumnRenamed('Id', 'SchoolId').withColumnRenamed('Name', 'SchoolName')\r\n",
        "# fill in US for country of location, temporarily - and add additional columns as placeholders for now\r\n",
        "dfSchool = dfSchool.withColumn('Country', F.lit('United States of America')).withColumn('Latitude', F.lit('')).withColumn('Longitude', F.lit(''))\r\n",
        "display(dfSchool)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 33,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:33:30.0501108Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:33:30.1578528Z",
              "execution_finish_time": "2023-01-11T18:33:30.6828889Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 33, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "2c8a6f08-c3b4-41af-babb-470dc7e342c5",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 2c8a6f08-c3b4-41af-babb-470dc7e342c5)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfSchool, 'stage2/Enriched/learning_analytics/v1.0/general/dim_School', 'stage3/Published/learning_analytics/v1.0/general/dim_School', primary_key='SchoolId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 34,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:34:05.7451868Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:34:05.8525779Z",
              "execution_finish_time": "2023-01-11T18:34:11.2772262Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 34, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:34:10,951 - OEA - INFO - Number of new inbound rows processed: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 71,
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.) Build dim_Meeting Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights activity data and Graph meeting_attendance_report pertaining to meetings. \r\n",
        "\r\n",
        "This table has one row per meeting recorded in both Insights and from the Graph query, with details around:\r\n",
        " - meeting ID,\r\n",
        " - date of the meeting,\r\n",
        " - meeting start time,\r\n",
        " - meeting end time, and\r\n",
        " - type of meeting.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Meeting```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')\r\n",
        "dfGraph_meetingAtten = oea.load('stage2/Refined/graph_api/v1.0/general/meeting_attendance_report')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 35,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:38:28.2697807Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:38:28.3977086Z",
              "execution_finish_time": "2023-01-11T18:38:30.2326893Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 35, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grab only the relevant data for the Meeting dimension table from Graph\r\n",
        "dfMeeting = dfGraph_meetingAtten.select('meetingId', 'meetingStartDateTime', 'meetingEndDateTime')\r\n",
        "dfMeeting = dfMeeting.groupBy('meetingId', 'meetingStartDateTime', 'meetingEndDateTime').count()\r\n",
        "dfMeeting = dfMeeting.drop('count')\r\n",
        "# grab only the relevant data for the Meeting table from Insights activity, and join the meeting type to the dfMeeting table\r\n",
        "df_metadata = dfInsights_activity.select('MeetingSessionId', 'MeetingType')\r\n",
        "df_metadata = df_metadata.groupBy('MeetingSessionId', 'MeetingType').count()\r\n",
        "df_metadata = df_metadata.drop('count')\r\n",
        "dfMeeting = dfMeeting.join(df_metadata, dfMeeting.meetingId == df_metadata.MeetingSessionId, how='inner').drop('MeetingSessionId')\r\n",
        "dfMeeting = dfMeeting.withColumnRenamed('meetingId', 'MeetingId').withColumnRenamed('meetingStartDateTime', 'StartTime').withColumnRenamed('meetingEndDateTime', 'EndTime')\r\n",
        "# add an additional column for the date of the meeting, and make the time columns only contain the times, without dates\r\n",
        "dfMeeting = dfMeeting.withColumn('MeetingDate', F.to_date(F.col('StartTime')))\r\n",
        "dfMeeting = dfMeeting.withColumn('STime', F.date_format('StartTime', 'HH:mm:ss')).withColumn('ETime', F.date_format('EndTime', 'HH:mm:ss')).drop('StartTime', 'EndTime')\r\n",
        "dfMeeting = dfMeeting.withColumnRenamed('STime', 'StartTime').withColumnRenamed('ETime', 'EndTime')\r\n",
        "dfMeeting = dfMeeting.select('MeetingId', 'MeetingDate', 'StartTime', 'EndTime', 'MeetingType')\r\n",
        "display(dfMeeting.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:38:35.0989628Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:38:35.2679008Z",
              "execution_finish_time": "2023-01-11T18:38:37.1071033Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "aaf219c2-b679-4cb4-add9-d8cebe2f8722",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, aaf219c2-b679-4cb4-add9-d8cebe2f8722)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfMeeting, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Meeting', 'stage3/Published/learning_analytics/v1.0/general/dim_Meeting', primary_key='MeetingId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 37,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:39:30.1549355Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:39:30.2643382Z",
              "execution_finish_time": "2023-01-11T18:39:37.3412897Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 37, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:39:35,853 - OEA - INFO - Number of new inbound rows processed: 261\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 77,
          "data": {
            "text/plain": "261"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.) Build dim_AssignmentStatus Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights activity data pertaining to the 4 possible assignment status's seen from the Insights activity table.\r\n",
        "\r\n",
        "This table has one row per assignment status from the activity table, with details on what the encoded AssignmentStatusId represents.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_AssignmentStatus```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:40:29.1623166Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:40:29.3216987Z",
              "execution_finish_time": "2023-01-11T18:40:29.4950172Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build assignment status table, based on the possible assignment actions from the Insights activity table\r\n",
        "dfAssignStatus = dfInsights_activity.select('AppName', 'Action')\r\n",
        "dfAssignStatus = dfAssignStatus.filter(dfAssignStatus['AppName'] == 'Assignments')\r\n",
        "dfAssignStatus = dfAssignStatus.groupBy('Action').count()\r\n",
        "dfAssignStatus = dfAssignStatus.withColumn('AssignmentStatusId', F.when(F.col('Action') == 'Assigned', '1').otherwise(F.when(F.col('Action') == 'Visited', '2').otherwise(F.when(F.col('Action') == 'Submitted', '3').otherwise(F.when(F.col('Action') == 'Returned', '4')))))\r\n",
        "dfAssignStatus = dfAssignStatus.drop('count').withColumnRenamed('Action', 'AssignmentStatus')\r\n",
        "display(dfAssignStatus.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 39,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:40:33.6720241Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:40:33.8068213Z",
              "execution_finish_time": "2023-01-11T18:40:34.9406229Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 39, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "eae073d2-6fda-4d00-8418-9cc3c6acf63b",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, eae073d2-6fda-4d00-8418-9cc3c6acf63b)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfAssignStatus, 'stage2/Enriched/learning_analytics/v1.0/general/dim_AssignmentStatus', 'stage3/Published/learning_analytics/v1.0/general/dim_AssignmentStatus', primary_key='AssignmentStatusId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 40,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:41:57.3351772Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:41:57.632421Z",
              "execution_finish_time": "2023-01-11T18:42:02.9588647Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 40, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:42:02,894 - OEA - INFO - Number of new inbound rows processed: 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 83,
          "data": {
            "text/plain": "4"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.) Build dim_Assignment Table\r\n",
        "Data aggregations and curation on Insights activity data pertaining to assignments. \r\n",
        "\r\n",
        "This table has one row per assignments from the Insights activity data, with details around the assigned and due dates.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Assignment```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfAssign = dfInsights_activity.filter(dfInsights_activity['Action'] == 'Assigned')\r\n",
        "dfAssign = dfAssign.groupBy('AssignmentId', 'DueDate', 'StartTime').count()\r\n",
        "dfAssign = dfAssign.drop('count').withColumn('AssignedDate', F.to_date(F.col('StartTime'))).withColumn('DueDate', F.to_date(F.col('DueDate'))).drop('StartTime')\r\n",
        "print('Number of Assignments Recorded:')\r\n",
        "print(dfAssign.count())\r\n",
        "display(dfAssign.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfTest = dfInsights_activity.groupBy('AssignmentId').count()\r\n",
        "print(dfTest.count())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfAssign, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Assignment', 'stage3/Published/learning_analytics/v1.0/general/dim_Assignment', primary_key='AssignmentId')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.) Build dim_SignalType Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights activity data pertaining to the various Insights activity SignalTypes. \r\n",
        "\r\n",
        "This table has one row per Insights SignalType from within the Activity data, with details and categorization of the SignalType.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_SignalType```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 41,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:42:07.7943482Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:42:07.9052063Z",
              "execution_finish_time": "2023-01-11T18:42:08.0679867Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 41, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Clean encoding process.\r\n",
        "# build signal type table, based on the possible signal types from the Insights activity table\r\n",
        "dfSignalType = dfInsights_activity.select('SignalType', 'AppName')\r\n",
        "dfSignalType = dfSignalType.groupBy('SignalType').count()\r\n",
        "# create a new column for categorizing the signals\r\n",
        "def SignalCat(SignalType):\r\n",
        "    if SignalType == 'PostChannelMessage':\r\n",
        "        res = 'Messaging'\r\n",
        "    elif SignalType == 'ReplyChannelMessage':\r\n",
        "        res = 'Messaging'\r\n",
        "    elif SignalType == 'VisitTeamChannel':\r\n",
        "        res = 'Messaging'\r\n",
        "    elif SignalType == 'ExpandChannelMessage':\r\n",
        "        res = 'Messaging'\r\n",
        "    elif SignalType == 'ReactedWithEmoji':\r\n",
        "        res = 'Messaging'\r\n",
        "    elif SignalType == 'Like':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'Unlike':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'FileAccessed':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'FileModified':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'FileDownloaded':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'FileUploaded':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'ShareNotificationRequested':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'AddedToSharedWithMe':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'CommentCreated':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'CommentDeleted':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'UserAtMentioned':\r\n",
        "        res = 'Files'\r\n",
        "    elif SignalType == 'Reflect':\r\n",
        "        res = 'Reflect'\r\n",
        "    elif SignalType == 'OneNotePageChanged':\r\n",
        "        res = 'Notebook'\r\n",
        "    elif SignalType == 'SubmissionEvent':\r\n",
        "        res = 'Assignments'\r\n",
        "    elif SignalType == 'AssignmentEvent':\r\n",
        "        res = 'Assignments'\r\n",
        "    elif SignalType == 'CallRecordSummarized':\r\n",
        "        res = 'TeamsMeeting'\r\n",
        "    else:\r\n",
        "        res = ''\r\n",
        "    return res\r\n",
        "\r\n",
        "def SignalTypeId(SignalType):\r\n",
        "    if SignalType == 'PostChannelMessage':\r\n",
        "        res = '000001'\r\n",
        "    elif SignalType == 'ReplyChannelMessage':\r\n",
        "        res = '000002'\r\n",
        "    elif SignalType == 'VisitTeamChannel':\r\n",
        "        res = '000003'\r\n",
        "    elif SignalType == 'ExpandChannelMessage':\r\n",
        "        res = '000004'\r\n",
        "    elif SignalType == 'ReactedWithEmoji':\r\n",
        "        res = '000005'\r\n",
        "    elif SignalType == 'Like':\r\n",
        "        res = '000010'\r\n",
        "    elif SignalType == 'Unlike':\r\n",
        "        res = '000011'\r\n",
        "    elif SignalType == 'FileAccessed':\r\n",
        "        res = '000020'\r\n",
        "    elif SignalType == 'FileModified':\r\n",
        "        res = '000021'\r\n",
        "    elif SignalType == 'FileDownloaded':\r\n",
        "        res = '000022'\r\n",
        "    elif SignalType == 'FileUploaded':\r\n",
        "        res = '000023'\r\n",
        "    elif SignalType == 'ShareNotificationRequested':\r\n",
        "        res = '000030'\r\n",
        "    elif SignalType == 'AddedToSharedWithMe':\r\n",
        "        res = '000040'\r\n",
        "    elif SignalType == 'CommentCreated':\r\n",
        "        res = '000050'\r\n",
        "    elif SignalType == 'CommentDeleted':\r\n",
        "        res = '000051'\r\n",
        "    elif SignalType == 'UserAtMentioned':\r\n",
        "        res = '000060'\r\n",
        "    elif SignalType == 'Reflect':\r\n",
        "        res = '000100'\r\n",
        "    elif SignalType == 'OneNotePageChanged':\r\n",
        "        res = '001000'\r\n",
        "    elif SignalType == 'SubmissionEvent':\r\n",
        "        res = '020000'\r\n",
        "    elif SignalType == 'AssignmentEvent':\r\n",
        "        res = '010000'\r\n",
        "    elif SignalType == 'CallRecordSummarized':\r\n",
        "        res = '100000'\r\n",
        "    else:\r\n",
        "        res = ''\r\n",
        "    return res\r\n",
        "# define the function/dataType\r\n",
        "new_f = F.udf(SignalCat, StringType())\r\n",
        "new_f2 = F.udf(SignalTypeId, StringType())\r\n",
        "  \r\n",
        "# add the new column\r\n",
        "dfSignalType = dfSignalType.withColumn(\"SignalCategory\", new_f('SignalType'))\r\n",
        "dfSignalType = dfSignalType.withColumn('SignalTypeId', new_f2('SignalType'))\r\n",
        "dfSignalType = dfSignalType.drop('count')\r\n",
        "display(dfSignalType)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 42,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:42:16.6760106Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:42:16.7989789Z",
              "execution_finish_time": "2023-01-11T18:42:23.6890996Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 42, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "27a7502d-cd08-4acc-bb9c-1d34fd86912a",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 27a7502d-cd08-4acc-bb9c-1d34fd86912a)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfSignalType, 'stage2/Enriched/learning_analytics/v1.0/general/dim_SignalType', 'stage3/Published/learning_analytics/v1.0/general/dim_SignalType', primary_key='SignalTypeId')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:43:18.7966597Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:43:18.9758505Z",
              "execution_finish_time": "2023-01-11T18:43:26.0038249Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 43, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:43:24,564 - OEA - INFO - Number of new inbound rows processed: 21\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 89,
          "data": {
            "text/plain": "21"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.) Build dim_Date Table\r\n",
        "\r\n",
        "Data aggregations and curation on Insights activity data pertaining to the various dates seen from activity. This table is used in the Power BI data model to connect dates seen tables, for filters in the dashboard.\r\n",
        "\r\n",
        "This table has one row per date seen from the activity data used, with details on that particular date.\r\n",
        "\r\n",
        "This table is then written out to```(stage2 and stage3)/(Enriched and Published)/learning_analytics/v1.0/general/dim_Date```."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfInsights_activity = oea.load('stage2/Refined/M365/v' + insights_version + '/general/activity')\r\n",
        "dfInsights_session = oea.load('stage2/Refined/M365/v' + insights_version + '/general/Session')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 44,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:44:13.4349506Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:44:13.5752226Z",
              "execution_finish_time": "2023-01-11T18:44:13.7307242Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 44, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Double-check that all columns are relevant and used in the dashboard.\r\n",
        "# extract dates to create Date table from Insights activity data\r\n",
        "dfDate = dfInsights_activity.select('StartTime')\r\n",
        "dfDate = dfDate.withColumn('Date', F.to_date(F.col('StartTime')))\r\n",
        "dfDate = dfDate.groupBy('Date').count()\r\n",
        "dfDate = dfDate.orderBy('Date')\r\n",
        "dfDate = dfDate.drop('count').withColumn('Year', F.year(F.col('Date'))).withColumn('Month', F.month(F.col('Date')))\r\n",
        "# extract the start date of the school year from the Insights Session table; in this case filter the table to get the start date of the school year\r\n",
        "list_of_schoolStartDate = dfInsights_session.filter(dfInsights_session['Name'] == '2021-2022').select('StartDate').collect()\r\n",
        "dfDate = dfDate.withColumn('SchoolYearStartDate', F.lit(list_of_schoolStartDate[0][0]))\r\n",
        "dfDate = dfDate.withColumn('SchoolYearStartDate', F.to_date(F.col('SchoolYearStartDate')))\r\n",
        "# calculate the number of months between activity date and the start date of the school year - then add 1 month of the floor to get the month of the school year\r\n",
        "dfDate = dfDate.withColumn('MonthOfSchoolYear', F.months_between(F.col('Date'), F.col('SchoolYearStartDate')))\r\n",
        "dfDate = dfDate.withColumn('MonthOfSchoolYear', F.floor(F.col('MonthOfSchoolYear'))).withColumn('MonthOfSchoolYear', F.col('MonthOfSchoolYear') + 1)\r\n",
        "# calculate the number of days between date and start of school year - then take the floor of the number of weeks (add 1 week) to get the week number of the school year\r\n",
        "dfDate = dfDate.withColumn('WeekOfSchoolYear', F.datediff(F.col('Date'), F.col('SchoolYearStartDate'))/7)\r\n",
        "dfDate = dfDate.withColumn('WeekOfSchoolYear', F.floor(F.col('WeekOfSchoolYear'))).withColumn('WeekOfSchoolYear', F.col('WeekOfSchoolYear') + 1)\r\n",
        "display(dfDate.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 45,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:44:18.2322449Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:44:18.3630636Z",
              "execution_finish_time": "2023-01-11T18:44:20.1895561Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 45, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "04f636ed-316a-4a5e-a5d7-c9b4a4433db7",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 04f636ed-316a-4a5e-a5d7-c9b4a4433db7)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the start date of the semester\r\n",
        "list_of_semesterStartDate = dfInsights_session.filter(dfInsights_session['Name'] == 'Spring2022').select('StartDate').collect()\r\n",
        "dfDate = dfDate.withColumn('SemesterStartDate', F.lit(list_of_semesterStartDate[0][0]))\r\n",
        "# calculate the number of months between activity date and the start date of the semester - then add 1 month of the floor to get the month of the semester\r\n",
        "dfDate = dfDate.withColumn('MonthOfSemester', F.months_between(F.col('Date'), F.col('SemesterStartDate')))\r\n",
        "dfDate = dfDate.withColumn('MonthOfSemester', F.floor(F.col('MonthOfSemester'))).withColumn('MonthOfSemester', F.col('MonthOfSemester') + 1)\r\n",
        "# calculate the number of days between date and start of semester - then take the floor of the number of weeks (add 1 week) to get the week number of the semester\r\n",
        "dfDate = dfDate.withColumn('WeekOfSemester', F.datediff(F.col('Date'), F.col('SemesterStartDate'))/7)\r\n",
        "dfDate = dfDate.withColumn('WeekOfSemester', F.floor(F.col('WeekOfSemester'))).withColumn('WeekOfSemester', F.col('WeekOfSemester') + 1)\r\n",
        "display(dfDate.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 46,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:44:24.2224245Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:44:24.355021Z",
              "execution_finish_time": "2023-01-11T18:44:26.1779532Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 46, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "da904f7c-2d2f-462e-9a1b-8b4c72817a41",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, da904f7c-2d2f-462e-9a1b-8b4c72817a41)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to Stage 3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "publish(dfDate, 'stage2/Enriched/learning_analytics/v1.0/general/dim_Date', 'stage3/Published/learning_analytics/v1.0/general/dim_Date', primary_key='Date')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "58",
              "statement_id": 47,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-11T18:45:31.8228336Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-11T18:45:31.9393446Z",
              "execution_finish_time": "2023-01-11T18:45:38.8472943Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 58, 47, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 18:45:37,705 - OEA - INFO - Number of new inbound rows processed: 34\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 97,
          "data": {
            "text/plain": "34"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}